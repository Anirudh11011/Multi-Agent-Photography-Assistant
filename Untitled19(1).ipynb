{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv6CcTjvxor7",
        "outputId": "a6f6b1e8-ca71-44d2-a96e-e3c89c99e5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.26)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.28.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (24.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading groq-0.28.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.28.0 langchain_groq-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community langgraph langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "import os"
      ],
      "metadata": {
        "id": "2ZdrZvLQ2xrb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "yqVfV3j73x5v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    raise RuntimeError(\"Please set the GROQ_API_KEY environment variable before running.\")"
      ],
      "metadata": {
        "id": "Ybd5nXI-15yq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from langchain_core.tools import tool, InjectedToolCallId\n",
        "from langgraph.prebuilt import InjectedState\n",
        "\n",
        "# ← UPDATED!\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.types import Command\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END"
      ],
      "metadata": {
        "id": "LNkyB1hqxxsO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq.chat_models import ChatGroq\n",
        "\n",
        "groq_model = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",  # ← replace with your actual Groq model ID\n",
        "    temperature=0.5,\n",
        "    max_tokens=None,\n",
        "    # api_key=os.environ[\"GROQ_API_KEY\"]  # Not needed if env var is already set\n",
        ")"
      ],
      "metadata": {
        "id": "bT5m7XEK4xnF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from langchain_core.tools import tool, InjectedToolCallId\n",
        "from langgraph.prebuilt import InjectedState\n",
        "\n",
        "# MessagesState still comes from langgraph.graph\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# Command now lives in langgraph.types (not langgraph.graph)\n",
        "from langgraph.types import Command\n",
        "\n",
        "def create_handoff_tool(*, agent_name: str, description: str | None = None):\n",
        "    \"\"\"\n",
        "    Returns a LangChain-Core Tool that, when invoked, emits a Command\n",
        "    telling LangGraph to 'goto=<agent_name>' and append a 'tool' message.\n",
        "    \"\"\"\n",
        "    tool_name = f\"transfer_to_{agent_name}\"\n",
        "    desc = description or f\"Hand off to {agent_name}.\"\n",
        "\n",
        "    # 1) Define the inner Python function that actually returns a Command.\n",
        "    def handoff_inner(\n",
        "        state: Annotated[MessagesState, InjectedState],\n",
        "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
        "    ) -> Command:\n",
        "        tool_message = {\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": f\"Transferring to {agent_name} for further handling.\",\n",
        "            \"name\": tool_name,\n",
        "            \"tool_call_id\": tool_call_id,\n",
        "        }\n",
        "        return Command(\n",
        "            goto=agent_name,  # jump to that agent node in the graph\n",
        "            update={**state, \"messages\": state[\"messages\"] + [tool_message]},\n",
        "            graph=Command.PARENT,  # after that agent runs, return to supervisor\n",
        "        )\n",
        "\n",
        "    # 2) Assign the function's __name__ to the desired tool name,\n",
        "    #    so that the Tool registry will pick up the correct name.\n",
        "    handoff_inner.__name__ = tool_name\n",
        "\n",
        "    # 3) Decorate with @tool(description=…), _without_ passing name=…,\n",
        "    #    because this version of `@tool` does not accept a name= argument.\n",
        "    decorated_tool = tool(description=desc)(handoff_inner)\n",
        "    return decorated_tool\n",
        "\n",
        "# Now instantiate the two tools:\n",
        "assign_to_scenario_agent = create_handoff_tool(\n",
        "    agent_name=\"scenario_analysis_agent\",\n",
        "    description=\"Analyze the shooting scenario and return context.\",\n",
        ")\n",
        "\n",
        "assign_to_specs_agent = create_handoff_tool(\n",
        "    agent_name=\"camera_specs_agent\",\n",
        "    description=\"Given the scenario, provide DSLR settings.\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "MdI0m_E4ykZM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt.chat_agent_executor import create_react_agent"
      ],
      "metadata": {
        "id": "3iXrztnr1Tf-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervisor_agent = create_react_agent(\n",
        "    model=groq_model,\n",
        "    tools=[assign_to_scenario_agent, assign_to_specs_agent],\n",
        "    prompt=(\n",
        "        \"You are a photography supervisor. You manage two agents:\\n\"\n",
        "        \"- scenario_analysis_agent: analyze a photography scenario (lighting, motion, environment).\\n\"\n",
        "        \"- camera_specs_agent: given a scenario, provide exact DSLR settings (lens, ISO, shutter speed, aperture).\\n\"\n",
        "        \"When the user gives input, decide which agent to call—do not answer yourself.\\n\"\n",
        "        \"Use exactly one handoff tool per turn: either transfer_to_scenario_analysis_agent \"\n",
        "        \"or transfer_to_camera_specs_agent.\\n\"\n",
        "        \"After that worker finishes, control returns here automatically.\\n\"\n",
        "        \"Do NOT do scenario analysis or give settings yourself—only hand off.\"\n",
        "    ),\n",
        "    name=\"supervisor\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "px30hKUT1XBt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_analysis_agent = create_react_agent(\n",
        "    model=groq_model,\n",
        "    tools=[],  # this worker never calls other tools\n",
        "    prompt=(\n",
        "        \"You are scenario_analysis_agent.\\n\"\n",
        "        \"Your job: Given a photography scenario (e.g. “shooting a hummingbird at dawn in low light”),\\n\"\n",
        "        \"analyze the environment, lighting conditions, subject motion, and dynamic range challenges.\\n\"\n",
        "        \"Do NOT supply camera settings—only describe what a photographer should watch out for.\"\n",
        "    ),\n",
        "    name=\"scenario_analysis_agent\",\n",
        ")"
      ],
      "metadata": {
        "id": "XNJuU2wD1Zcf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "camera_specs_agent = create_react_agent(\n",
        "    model=groq_model,\n",
        "    tools=[],\n",
        "    prompt=(\n",
        "        \"You are camera_specs_agent.\\n\"\n",
        "        \"Your job: Given a photographer’s scenario or prior analysis, provide a complete DSLR configuration:\\n\"\n",
        "        \"- Lens (e.g. 24-70mm f/2.8)\\n\"\n",
        "        \"- Aperture\\n\"\n",
        "        \"- Shutter Speed\\n\"\n",
        "        \"- ISO\\n\"\n",
        "        \"- White Balance\\n\"\n",
        "        \"- Autofocus Mode (e.g. AI-Servo single-point)\\n\"\n",
        "        \"Be as precise as possible. Assume a full-frame DSLR by default.\"\n",
        "    ),\n",
        "    name=\"camera_specs_agent\",\n",
        ")"
      ],
      "metadata": {
        "id": "yvxsM2VF1bRx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervisor_graph = (\n",
        "    StateGraph(MessagesState)\n",
        "    .add_node(\n",
        "        supervisor_agent,\n",
        "        destinations=(\"scenario_analysis_agent\", \"camera_specs_agent\", END),\n",
        "    )\n",
        "    .add_node(scenario_analysis_agent)\n",
        "    .add_node(camera_specs_agent)\n",
        "    .add_edge(START, \"supervisor\")\n",
        "    .add_edge(\"scenario_analysis_agent\", \"supervisor\")\n",
        "    .add_edge(\"camera_specs_agent\", \"supervisor\")\n",
        "    .compile()\n",
        ")\n"
      ],
      "metadata": {
        "id": "3ocOpEAY6N3u"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────\n",
        "# Cell: Stream and “manually” print both dicts and AIMessage objects\n",
        "# ─────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n=== Streaming a Sample Prompt (robust manual print) ===\\n\")\n",
        "\n",
        "# Initialize a variable to hold the very last assistant output (if you need it later)\n",
        "final_assistant_output = None\n",
        "\n",
        "for chunk in supervisor_graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"I want to photograph a insect on leaf in medium light. \"\n",
        "                    \"What lens and settings should I use?\"\n",
        "                ),\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    # Each chunk is a dict whose key is the name of the agent (or 'tool') that just produced output.\n",
        "    # Example keys might be [\"supervisor\"] for the tool message, then [\"camera_specs_agent\"] for the actual specs.\n",
        "    present_keys = list(chunk.keys())\n",
        "    print(f\"Chunk produced by: {present_keys}\\n\")\n",
        "\n",
        "    # Iterate over each key→payload pair (usually there's only one key per chunk)\n",
        "    for agent_name, agent_payload in chunk.items():\n",
        "        # We expect agent_payload to be a dict containing a \"messages\" list\n",
        "        if not (isinstance(agent_payload, dict) and \"messages\" in agent_payload):\n",
        "            print(f\"  (Skipping payload for {agent_name}: {agent_payload})\\n\")\n",
        "            continue\n",
        "\n",
        "        history = agent_payload[\"messages\"]\n",
        "        if not history:\n",
        "            print(f\"  ({agent_name} has an empty message history)\\n\")\n",
        "            continue\n",
        "\n",
        "        last_msg = history[-1]\n",
        "\n",
        "        # If it's a plain dict (e.g. {'role':..., 'content':...}), pull out keys directly:\n",
        "        if isinstance(last_msg, dict):\n",
        "            role = last_msg.get(\"role\", \"<unknown>\")\n",
        "            content = last_msg.get(\"content\", \"\")\n",
        "        else:\n",
        "            # It's likely an AIMessage or HumanMessage (or similar LangChain BaseMessage).\n",
        "            # Try to grab .role first; if not present, fall back to .type (or use \"<assistant>\" as default).\n",
        "            role = getattr(last_msg, \"role\", None) or getattr(last_msg, \"type\", \"<assistant>\")\n",
        "            # The message’s text is in .content\n",
        "            content = getattr(last_msg, \"content\", str(last_msg))\n",
        "\n",
        "        print(f\"{agent_name} → {role}:\\n{content}\\n---\\n\")\n",
        "\n",
        "        # If this is an assistant response, remember it\n",
        "        if role.lower() in (\"assistant\", \"ai\", \"tool\"):  # adjust as needed\n",
        "            final_assistant_output = content\n",
        "\n",
        "# After the loop, `final_assistant_output` holds the last assistant (or tool) content, if you need it.\n",
        "print(\"✅ Streaming complete.\")\n",
        "print(\"Final assistant output (if any):\")\n",
        "print(final_assistant_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCDueGGw6VbB",
        "outputId": "9e9bf4e3-4b18-45ab-a4ab-798f64c714ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Streaming a Sample Prompt (robust manual print) ===\n",
            "\n",
            "Chunk produced by: ['supervisor']\n",
            "\n",
            "supervisor → tool:\n",
            "Transferring to camera_specs_agent for further handling.\n",
            "---\n",
            "\n",
            "Chunk produced by: ['camera_specs_agent']\n",
            "\n",
            "camera_specs_agent → ai:\n",
            "Given your scenario of photographing an insect on a leaf in medium light, I recommend the following DSLR configuration:\n",
            "\n",
            "- **Lens:** A macro lens with a focal length of 100mm or 105mm, such as the Canon MP-E 65mm f/2.8 1-5x Macro or the Nikon AF-S VR Micro-NIKKOR 105mm f/2.8G IF-ED. However, if you don't have a macro lens, a close-up lens or a telephoto zoom lens (e.g., 70-200mm) can also be used with a extension tubes or a bellows to achieve the desired close-up effect.\n",
            "\n",
            "- **Aperture:** A medium aperture of f/8 to f/11 to ensure a large depth of field, allowing both the insect and the leaf to be in focus.\n",
            "\n",
            "- **Shutter Speed:** A shutter speed of 1/125s to 1/250s to freeze the insect's movement and capture a sharp image.\n",
            "\n",
            "- **ISO:** An ISO of 400 to 800 to minimize noise and ensure a clean image in medium light.\n",
            "\n",
            "- **White Balance:** Auto White Balance (AWB) or Daylight White Balance (5600K) to capture the natural colors of the scene.\n",
            "\n",
            "- **Autofocus Mode:** Continuous Autofocus (AI-Servo) with a single-point autofocus or a focus lock to track the insect's movement and maintain focus on it.\n",
            "\n",
            "Please note that these settings can be adjusted based on your specific camera model and the available light conditions.\n",
            "---\n",
            "\n",
            "Chunk produced by: ['supervisor']\n",
            "\n",
            "supervisor → ai:\n",
            "\n",
            "---\n",
            "\n",
            "✅ Streaming complete.\n",
            "Final assistant output (if any):\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frontend\n"
      ],
      "metadata": {
        "id": "XIgOKlPeW5Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio openai-whisper torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEzx6bw-W2sO",
        "outputId": "5afbd3e8-7d80-4035-8d61-9091d7d0b120"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/800.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=46820218a1fb48edec92b63818588b902fd0ae69d88dbf7a4aca6e46f227e509\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "QOMPJBucW9ZN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Whisper model once\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "def respond_to_input(text):\n",
        "    response_parts = []\n",
        "\n",
        "    for chunk in supervisor_graph.stream(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ):\n",
        "        for agent_name, agent_payload in chunk.items():\n",
        "            if not (isinstance(agent_payload, dict) and \"messages\" in agent_payload):\n",
        "                continue\n",
        "\n",
        "            history = agent_payload[\"messages\"]\n",
        "            if not history:\n",
        "                continue\n",
        "\n",
        "            last_msg = history[-1]\n",
        "            if isinstance(last_msg, dict):\n",
        "                role = last_msg.get(\"role\", \"<unknown>\")\n",
        "                content = last_msg.get(\"content\", \"\")\n",
        "            else:\n",
        "                role = getattr(last_msg, \"role\", None) or getattr(last_msg, \"type\", \"<assistant>\")\n",
        "                content = getattr(last_msg, \"content\", str(last_msg))\n",
        "\n",
        "            if role.lower() != \"user\":\n",
        "                response_parts.append(content)\n",
        "\n",
        "    final_output = \"\\n\".join(response_parts).strip()\n",
        "\n",
        "    with open(\"latest_response.txt\", \"w\") as f:\n",
        "        f.write(final_output)\n",
        "\n",
        "    return final_output or \"No response received.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transcribe_audio(audio):\n",
        "    if audio is None:\n",
        "        return \"\"\n",
        "    result = model.transcribe(audio)\n",
        "    return result[\"text\"]\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".custom-bg {\n",
        "    background-image: url('https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fgetwallpapers.com%2Fwallpaper%2Ffull%2F1%2Fb%2F9%2F464324.jpg&f=1&nofb=1&ipt=6f4e0bac0d27c85a6d5c30b5e2fdfa202d9123ec1a66890cd365be59c7afee43');\n",
        "    background-size: cover;\n",
        "    background-position: center;\n",
        "    min-height: 100vh;\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    align-items: center;\n",
        "}\n",
        "\n",
        ".glass-card {\n",
        "    background: rgba(255, 255, 255, 0.2);\n",
        "    backdrop-filter: blur(10px);\n",
        "    padding: 40px;\n",
        "    border-radius: 20px;\n",
        "    width: 90%;\n",
        "    max-width: 500px;\n",
        "    box-shadow: 0 8px 32px rgba(0,0,0,0.3);\n",
        "}\n",
        "\n",
        "#mytextbox {\n",
        "    background-color: #cccccc !important;\n",
        "    padding: 12px !important;\n",
        "    border-radius: 10px !important;\n",
        "}\n",
        "\n",
        "#mytextbox textarea,\n",
        "#mytextbox input {\n",
        "    background: transparent !important;\n",
        "    border: none !important;\n",
        "    resize: none !important;\n",
        "    color: #000 !important;\n",
        "    font-size: 16px !important;\n",
        "    line-height: 1.4 !important;\n",
        "}\n",
        "\n",
        "#mytextbox textarea::placeholder,\n",
        "#mytextbox input::placeholder {\n",
        "    color: #e0e0e0 !important;\n",
        "}\n",
        "\n",
        "#mytextbox label {\n",
        "    display: block;\n",
        "    margin-bottom: 6px;\n",
        "    color: #fff !important;\n",
        "    font-weight: 500;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    with gr.Row(elem_classes=\"custom-bg\"):\n",
        "        with gr.Column(elem_classes=\"glass-card\"):\n",
        "            gr.Markdown(\"## 💬 Stylish Input + 🗣️ Speech-to-Text\")\n",
        "            input_text = gr.Textbox(label=\"Type something…\", elem_id=\"mytextbox\")\n",
        "            submit_btn = gr.Button(\"Submit\")\n",
        "            gr.Markdown(\"## ---------------------------Or----------------------------\")\n",
        "            audio_input = gr.Audio(sources=\"microphone\", type=\"filepath\", label=\"Speak instead\")\n",
        "            transcribe_btn = gr.Button(\"🎙️ Transcribe\")\n",
        "\n",
        "            output_text = gr.Textbox(label=\"Output\", elem_id=\"mytextbox\", interactive=False)\n",
        "\n",
        "            # FIXED: Transcribed text now goes to output_text\n",
        "            transcribe_btn.click(\n",
        "    fn=transcribe_audio,\n",
        "    inputs=audio_input,\n",
        "    outputs=input_text  # populate the input box\n",
        ")\n",
        "\n",
        "            # Textbox input still controls output normally\n",
        "            submit_btn.click(fn=respond_to_input, inputs=input_text, outputs=output_text)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "AMhqVrSfXBsz",
        "outputId": "f836a542-50b7-4243-85d3-e5ea5d190a50"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1173ebc66ec70af4b9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1173ebc66ec70af4b9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}